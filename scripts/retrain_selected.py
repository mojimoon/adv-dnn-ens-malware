from learner import model_scope_dict
from tools import utils
import numpy as np
'''
Perform retraining with the candidate set selected with test selection metrics
'''

import sys
import logging
import os
import csv

logger = logging.getLogger(__name__)
logger.setLevel(level=logging.DEBUG)
# StreamHandler
stream_handler = logging.StreamHandler(sys.stdout)
stream_handler.setLevel(level=logging.DEBUG)
logger.addHandler(stream_handler)
# FileHandler
file_handler = logging.FileHandler('retrain_selected.log')
file_handler.setLevel(level=logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(message)s')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)


targeted_model_names_dict = model_scope_dict.copy()
targeted_model_name = 'basic_dnn'
targeted_model = targeted_model_names_dict[targeted_model_name](mode='test')

pwd = os.getcwd()
root = os.path.dirname(pwd)
log_path = os.path.join(pwd, 'results', 'retrain_selected_2.csv')
feat_path = os.path.join(root, 'AndroZoo', 'attack', 'adversarial_samples')

# test on 11 different test set distributions
# id_dist = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] #, 1.0]
# ood_dist = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1] #, 0.0]
id_dist = [1.0]
ood_dist = [0.0]
# Note that pristine feats are the same under each attack methods.
pristine_feat_path = os.path.join(feat_path, 'fgsm', 'pristine_l-infinity.data')
pristine_feat = utils.readdata_np(pristine_feat_path)

#todo: change to other attack methods's perturbed feat path
all_methods = ['fgsm', 'PGD-linf', 'PGD-l2', 'PGD-l1', 'PGD-Adam', 'GDKDE', 'BCA_K', 'BGA_K', 'GROSSE', 'JSMA', 'MAX', 'MIMICRY', 'POINTWISE']
all_perturbed_paths = [
    os.path.join(feat_path, 'fgsm', 'fgsm_l-infinity.data'),
    os.path.join(feat_path, 'pgdlinf', 'pgdlinf_l-infinity.data'),
    os.path.join(feat_path, 'pgdl2', 'pgdl2_l2.data'),
    os.path.join(feat_path, 'pgdl1', 'pgdl1_.data'),
    os.path.join(feat_path, 'pgd_adam', 'pgd_adam_.data'),
    os.path.join(feat_path, 'gdkde', 'gdkde_.data'),
    os.path.join(feat_path, 'bca_k', 'bca_k_.data'),
    os.path.join(feat_path, 'bga_k', 'bga_k_.data'),
    os.path.join(feat_path, 'grosse', 'grosse_.data'),
    os.path.join(feat_path, 'jsma', 'jsma_.data'),
    os.path.join(feat_path, 'max', 'max_.data'),
    os.path.join(feat_path, 'mimicry', 'mimicry_.data'),
    os.path.join(feat_path, 'pointwise', 'pointwise_.data'),
]

# experiment variables

tests = [
    {
        'selection_metric': ['entropy', 'deepgini', 'dat'],
        'budget': [0.1, 0.2, 0.3],
        'epochs': [25, 30],
        'id_ratio': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
        'method': ['MIMICRY']
    },
    {
        'selection_metric': ['entropy', 'deepgini'],
        'budget': [0.1, 0.2, 0.3],
        'epochs': [25, 30],
        'id_ratio': [1.0],
        'method': ['MIMICRY']
    },
    {
        'selection_metric': ['none'],
        'budget': [1.0],
        'epochs': [25, 30],
        'id_ratio': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
        'method': ['MIMICRY']
    },
    {
        'selection_metric': ['entropy', 'deepgini', 'dat'],
        'budget': [0.1, 0.2, 0.3],
        'epochs': [25, 30],
        'id_ratio': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
        'method': ['POINTWISE']
    },
    {
        'selection_metric': ['entropy', 'deepgini'],
        'budget': [0.1, 0.2, 0.3],
        'epochs': [25, 30],
        'id_ratio': [1.0],
        'method': ['POINTWISE']
    },
    {
        'selection_metric': ['none'],
        'budget': [1.0],
        'epochs': [25, 30],
        'id_ratio': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
        'method': ['POINTWISE']
    },
]

def write_to_csv(output_file, data):
    file_exists = os.path.isfile(output_file)
    with open(output_file, 'a') as csvfile:
        fieldnames = ['method', 'selection_metric', 'epochs', 'id_ratio', 'budget', 'original_acc', 'retrain_acc', 'acc_improvement']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        writer.writerow(data)

def retrain_selected(test_params):
    selection_metric = test_params['selection_metric']
    bg = test_params['budget']
    retrain_epochs = test_params['epochs']
    id_dist = test_params['id_ratio']
    methods = test_params['method']
    perturbed_paths = [all_perturbed_paths[all_methods.index(method)] for method in methods]

    for method, perturbed_path in zip(methods, perturbed_paths):
        if perturbed_path is None:
            continue
        perturbed_feat = utils.readdata_np(perturbed_path)
        for budget in bg:
            for metric in selection_metric:
                for epochs in retrain_epochs:
                    accuracies, ori_accs = [], []
                    for id_ratio in id_dist:
                        id_size = int(pristine_feat.shape[0]*id_ratio)
                        ood_size = int(pristine_feat.shape[0] - id_size)
                        id_feat = pristine_feat[:id_size]

                        # split id feat into two sets
                        candidate_set_id = id_feat[:int(id_size*0.5)]
                        test_set_id = id_feat[int(id_size*0.5):]

                        ood_feat = perturbed_feat[id_size:]
                        candidate_set_ood = ood_feat[:int(ood_size*0.5)]
                        test_set_ood = ood_feat[int(ood_size*0.5):]


                        # reconstruct hybrid test set and hybrid candidate set
                        hybrid_candidate_set = np.concatenate((candidate_set_id, candidate_set_ood), axis=0)
                        hybrid_test_set = np.concatenate((test_set_id, test_set_ood), axis=0)
                        labels = np.ones(hybrid_test_set.shape[0])
                        test_labels = np.ones(hybrid_test_set.shape[0])
                        ori_acc = targeted_model.test_rpst(testX=hybrid_test_set, testy=labels, is_single_class=True)

                        selected_candidateX, selected_candidatey = None, None

                        if budget == 1.0:
                            selected_candidateX = hybrid_candidate_set
                            selected_candidatey = labels
                        else:
                            selected_candidateX, selected_candidatey = targeted_model.selection(budget=budget, candidateX=hybrid_candidate_set,
                                                                        candidateX_id=candidate_set_id, candidatey=labels, candidatey_id=np.ones(candidate_set_id.shape[0]),
                                                                                            hybrid_test=hybrid_test_set, hybrid_testy=test_labels, metric=metric, id_ratio=id_ratio)

                        print('############ Start Retraining #############')
                        acc = targeted_model.retrain(candidateX=selected_candidateX, candidatey=selected_candidatey, testX=hybrid_test_set, testy=labels, epochs=epochs)
                        accuracies.append(acc*100)
                        ori_accs.append(ori_acc*100)
                    for i in range(len(id_dist)):
                        write_to_csv(log_path, {'method': method, 'id_ratio': id_dist[i], 'budget': budget, 'selection_metric': metric,  'original_acc': ori_accs[i], 'retrain_acc': accuracies[i], 'acc_improvement': accuracies[i]-ori_accs[i], 'epochs': epochs})

def main():
    for test in tests:
        retrain_selected(test)

if __name__ == '__main__':
    main()